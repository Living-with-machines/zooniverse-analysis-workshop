{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Zooniverse classification and subject data into a CSV\n",
    "\n",
    "This Notebook is designed to read the classifications and subject files, which you will have exported from Zooniverse. After it reads the files, the notebook will perform the following tasks:\n",
    "\n",
    "1. It combines the information from:  \n",
    "    (a) the **classifications** file, which contains all the annotations users have provided for a given workflow's subjects, with  \n",
    "    (b) the information from the **subjects** file, which documents the information about the dataset used for a specific task.\n",
    "\n",
    "2. It processes the columns that contain information in JSON format and transforms them (and the rest of the data) into spreadsheet columns for ease of use. \n",
    "\n",
    "3. It removes any Personally Identifiable Information, such as usernames and IP addresses.\n",
    "\n",
    "4. It produces a .csv file that can be opened in any spreadsheet tool (Microsoft Excel, Apple Numbers, etc.) for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting set up\n",
    "\n",
    "In the next cell, you will specify where the data that you want to process is located. These should be two paths on your drive, which can be _relative_ (to where this notebook is located) or _absolute_ (a specified full path to each file).\n",
    "\n",
    "#### Example 1: Sample data\n",
    "\n",
    "If you do not have any downloaded data, but want to use the sample data that we have provided (located in the same folder as this notebook), you should put these two _relative_ paths in the next cell:\n",
    "\n",
    "```py\n",
    "classifications_file = \"sample_data/test_classifications.csv\"\n",
    "subjects_file = \"sample_data/test_subjects.csv\"\n",
    "```\n",
    "\n",
    "Pandas is a very flexible Python package, which can also accept valid URLs, so you could also write:\n",
    "\n",
    "```py\n",
    "classifications_file = \"https://raw.githubusercontent.com/Living-with-machines/zooniverse-analysis-workshop/main/sample_data/test_classifications.csv\"\n",
    "subjects_file = \"https://raw.githubusercontent.com/Living-with-machines/zooniverse-analysis-workshop/main/sample_data/test_subjects.csv\"\n",
    "```\n",
    "\n",
    "#### Example 2: Downloaded data in Downloads folder\n",
    "\n",
    "If you have downloaded the files as `classifications.csv` and `subjects.csv` on a Mac and they are located in your `Downloads` folder, in the next cell you would put two _absolute_ paths:\n",
    "\n",
    "```py\n",
    "classifications_file = \"/Users/<your-username>/Downloads/classifications.csv\"\n",
    "subjects_file = \"/Users/<your-username>/Downloads/subjects.csv\"\n",
    "```\n",
    "\n",
    "\n",
    "#### Example 3: Data in your Google Drive\n",
    "\n",
    "```py\n",
    "# Code originally from https://github.com/kingsdigitallab/lwm-davizct\n",
    "# Update the paths to match the location of files on your Google Drive\n",
    "# Running this code will trigger a dialogue to allow access to your Google Drive\n",
    "ipython = get_ipython()\n",
    "IN_COLAB = \"google.colab\" in str(ipython)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    data_path = \"drive/My Drive/Zooniverse_exports\"\n",
    "    sources_path = \"drive/My Drive/Zooniverse_exports\" # not needed in this notebook?\n",
    "    subjects_path = \"drive/My Drive/Zooniverse_exports\" # not needed in this notebook?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is your turn!\n",
    "\n",
    "**Fill in the file locations in this cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_file = \"sample_data/test_classifications.csv\"\n",
    "subjects_file = \"sample_data/test_subjects.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our definitions\n",
    "\n",
    "**Now, we are ready to get started working with our data.**\n",
    "\n",
    "First, we need to import the packages that we are going to use in the script below. Most of them are built-in to Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our main data\n",
    "\n",
    "Next, it's time to read in the main CSV data as a pandas DataFrame, using the handy method `.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(classifications_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a handy method to look at the data that we have just imported, called `.head(num)`, where `num` should be replaced with the amount of rows in the frame that you want to see.\n",
    "\n",
    "We run it below to see what our imported data looks like unprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in our preview, the `classification_id` is a column, which is unique for each classification. It can thus be used as an \"index\" for the DataFrame, a nice way of querying the frame by individual IDs.\n",
    "\n",
    "Here's how we can set the index on our `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"classification_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Python and Pandas are both \"succeeding silently\", it can be good to sometimes have what we call a \"reality check\" to ensure that we have the result that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure correct encoding\n",
    "\n",
    "Next, we will want to have a look at the encoding of each column.\n",
    "\n",
    "In the \"reality check\" above, you can see that the columns `subject_data`, `annotations`, and `metadata` are structured as JSON (JavaScript Object Notation), which the DataFrame parser cannot interpret on its own, which is why we have to help it using the following parsing of those specific columns.\n",
    "\n",
    "If you use Pandas' `.apply()` method (which can be applied on a column or on the entire DataFrame), you can help the software interpret the data in the columns. Here, we pass a function (`json.loads`, i.e. the `loads` function from Python's built-in `json` package) to each of the columns containing JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"metadata\"] = df[\"metadata\"].apply(json.loads)\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we might want to do a \"reality check\". To look at a specific part of a dataframe we can \"slice it\" by passing it a list (note the double `[[` and `]]` on each side of the selector) of the columns we want to look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"annotations\", \"metadata\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!\n",
    "\n",
    "In the following two sections, we will process the `metadata`, and `annotations` column respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract classification metadata (`metadata`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to do something similar to what we did with the subjects above to process the metadata for each classification.\n",
    "\n",
    "Because the data in the `metadata` column has nested JSON data (that is, objects and lists that are wrapped inside each other), we want to use the particular `json_normalize` method.\n",
    "\n",
    "Here's an example of what one `metadata` row looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.metadata[df.metadata.head(1).index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a separate DataFrame from the normalised JSON data in the columns using the json_normalize method and use `.head()` to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.json_normalize(df[\"metadata\"])\n",
    "\n",
    "df_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the shape of the classification metadata DataFrame (`df_metadata`) and the main classification DataFrame (`df`) are the same, we can apply the `set_index` method to the metadata to get the `classification_id` as index on the `df_metadata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.set_index(df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reality check time again — good practice, keep track of what you're doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract annotations (`annotations`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final information we want to extract and process from the main DataFrame are the annotations for each classification.\n",
    "\n",
    "In order to do so, we must first preprocess the JSON data that we have available, extracting a list of only the information we want (the \"annotation values\"). We set up a function that processes each individual row, and then apply it to each row, using the `.apply` method, which we have used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation_values(annotation_row):\n",
    "    \"\"\"\n",
    "    Takes an annotation row, which contains a list of tasks with values in dictionary {task, task_label, value}\n",
    "    and extracts the `value` for each `task`, disregarding the `task_label` and returns them as a dictionary,\n",
    "    for easy insertion into a DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_dictionaries = [{task_data.get('task'): task_data.get('value')} for task_data in annotation_row]\n",
    "    return dict(ChainMap(*extracted_dictionaries))\n",
    "\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(extract_annotation_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similarly to section 4 above, we loop through each row (of JSON data - contained in `json_data`) and extract the classification_id and annotations for each of them, which we then add on our new DataFrame `df_annotations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = pd.DataFrame()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    s = pd.Series(row.annotations, name=index)\n",
    "    df_annotations = pd.concat([df_annotations, s], axis=1)\n",
    "    \n",
    "df_annotations = df_annotations.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the `.head()` method again to check our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Zooniverse subjects (`subject_data`)\n",
    "\n",
    "Next, we want to read in the `subjects.csv` file above, so we know what files each of the classifications were done on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects = pd.read_csv(subjects_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at our data using the `.head()` method that we learned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the classifications, we can see from this preview that the `subject_id` is a column, which is unique for each subject. Similarly to what we did with the classifications above, we can turn it into an \"index\" for the DataFrame, a nice way of querying the frame by individual IDs. Again, here's how we'd do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.set_index(\"subject_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the preview, there are some quirky aspects of the `subjects.csv` file from Zooniverse. Two things we might want to address here is that the `metadata` column is formatted as embedded JSON data, and so is the `locations` column.\n",
    "\n",
    "The `metadata` column corresponds to the data from any manifest file that you uploaded with your subject sets, so it often contains valuable information for us to have with our classifications.\n",
    "\n",
    "The `locations` column contains the URL to all of the images that each subject links to. For our projects, we might just have one location, but not necessarily: subjects can contain multiple images!\n",
    "\n",
    "For our purposes, we want to create a more readable list of locations and extract the metadata into its own columns. Let's go ahead and do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by making sure that both columns are formatted correctly, as we did with the classifications' metadata and annotations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects[\"metadata\"] = df_subjects[\"metadata\"].apply(json.loads)\n",
    "df_subjects[\"locations\"] = df_subjects[\"locations\"].apply(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the `locations` column. Since the data is structured as a Python dictionary `{id: \"URL\"}`, we can get a list of all the URLs by using Python's `dict` type's built-in `.values()` method. Here, we put that method into a custom function, which takes any row from the `subjects` DataFrame, makes a list from its values, and then joins them together with a comma separation (see the `\", \".join()` syntax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_as_list(row):\n",
    "    return \", \".join(list(row.values()))\n",
    "\n",
    "df_subjects[\"locations_list\"] = df_subjects[\"locations\"].apply(locations_as_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to save some space, we can remove the old `locations` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects = df_subjects.drop([\"locations\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will turn to the subjects' `metadata` column.\n",
    "\n",
    "Similarly to the extraction of classifications metadata above, we will create a separate DataFrame from the normalised JSON data from the subjects column using the `json_normalize` method and then use `.head()` to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects_metadata = pd.json_normalize(df_subjects[\"metadata\"])\n",
    "\n",
    "df_subjects_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since we know that the shape of the subjects' metadata DataFrame (`df_subjects_metadata`) and the main subject DataFrame (`df_subject`) are the same, we can apply the `set_index` method to the metadata to get the `subject_id` as index on the `df_subjects_metadata`.\n",
    "\n",
    "After, we'll run the `.head()` reality check, to make sure all is well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects_metadata.set_index(df_subjects.index, inplace=True)\n",
    "\n",
    "df_subjects_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've now extracted the `metadata` column into a separate DataFrame, let's go ahead and remove the column from the original or \"main\" `df_subjects` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects = df_subjects.drop(\"metadata\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really great feature of DataFrames is that you can join them back together, as long as the `pandas` library has an easy way of matching the two DataFrames (or more!) together. A joined index, such as we have for `df_subjects` and `df_subjects_metadata` is a great way to do so.\n",
    "\n",
    "In order to get one large `df_subjects` DataFrame that contains all the data for each subject, we can thus run the `.join` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects = df_subjects.join(df_subjects_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see that `pandas` succeeds silently, so let's do a reality check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is one large (but handy) DataFrame!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining all the data back together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final step, we want to join the data back together into a new main DataFrame, `df_final`, from which we will remove all the personal data so the dataset can be shared publicly, if we want.\n",
    "\n",
    "To recap, here are the four DataFrames that we have created thus far:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main DataFrame (`df`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subjects DataFrame (`df_subjects`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata DataFrame (`df_metadata`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations DataFrame (`df_annotations`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop existing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the summary above, of the main DataFrame (`df`), it now doubles the information in `df_subjects`, `df_metadata`, and `df_annotations`. So before we do anything else, we will drop the columns that we have now processed from `df` to reduce the size of `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"subject_data\", \"metadata\", \"annotations\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redact personal information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to ensure that we have anonymised the user data.\n",
    "\n",
    "Here, we have created our own function, `redact_username`, which uses a cryptographic hash method from Python's built-in `hashlib` library called `sha256`. You can read more about [the algorithm's history on Wikipedia](https://en.wikipedia.org/wiki/SHA-2) if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_username(row):\n",
    "    return hashlib.sha256(str(row).encode()).hexdigest() if not pd.isna(row) else None\n",
    "    \n",
    "df[\"user_name_redacted\"] = df[\"user_name\"].apply(redact_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also be interested in knowing whether a user was logged in or not when they annotated the subject. In order to preserve that information, we create another custom function, `user_was_logged_in`, which uses the information from the `user_name` column (which has a value like `not-logged-in-98ff168ef257e2fd9d4a` if the user was not logged in) to extract a `True` or `False` value (also called a \"boolean\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_was_logged_in(row):\n",
    "    return \"not-logged-in\" not in row if not pd.isna(row) else False\n",
    "\n",
    "df[\"user_logged_in\"] = df[\"user_name\"].apply(user_was_logged_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to drop columns that contain personal identifying information - usernames, IDs and IP addresses - from the main DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"user_id\", \"user_name\", \"user_ip\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all the data\n",
    "\n",
    "Now, it's time to join all the DataFrames into one new `df_final` frame.\n",
    "\n",
    "Adding the metadata and the annotations on the main `df` is an easy thing, since all of them share index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.join(df_metadata).join(df_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick reality check to make sure that all the data is there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. But now we want to join our subjects on the `df_final` table as well. It's not entirely as easy as the previous multiple join task that we just did here.\n",
    "\n",
    "This time, because we're not using data points that are uniquely connected, one-to-one (one subject, in fact, will have many classifications), we will end up with duplicated information in the table. In order to join the main DataFrame with the subject DataFrame `df_subjects`, we need to make one intermediary step: We need to make both index columns available in each of the DataFrames.\n",
    "\n",
    "_Why do we need to make `df_final`'s index a column?_ We will _merge_ two DataFrames, which is a function that will discard both indices of the merging DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects[\"subject_ids\"] = df_subjects.index\n",
    "df_final[\"classification_id\"] = df_final.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the merge, we also want to make sure that the two matching columns on both DataFrames are of the same (correct) type—otherwise Pandas will have trouble finding the matching subject row for each classification.\n",
    "\n",
    "For that, we use the built-in method on each DataFrame, `.astype()` which \"casts\" a column as a type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects[\"subject_ids\"] = df_subjects[\"subject_ids\"].astype(int)\n",
    "df_final[\"subject_ids\"] = df_final[\"subject_ids\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be able to use the `merge` function from Pandas central library to join the two on the column that is shared between the two DataFrames (in our case `subject_ids`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_final, df_subjects, on=\"subject_ids\", how=\"left\", suffixes=(\"_classification\", \"_subject\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can make the `classification_id` column an index again, on our final DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.set_index(\"classification_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last reality check, before we're ready to export!\n",
    "\n",
    "This time, however, we set the max column to be displayed to `None` because we're interested in seeing all the columns. Then we use our now familiar `.head()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export!\n",
    "\n",
    "In this final step, we can export the file to whatever format that we'd like to use.\n",
    "\n",
    "In the case below, we opt for `.csv`, which is a common, open file format that can be easily opened in Microsoft Excel for further processing. It can also be imported into visualisation software, such as Power BI, or websites like ObservableHQ. On Colab, the file will be stored in a temporary directory. Once you've run the code below, click the 'folder' icon to view it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"combined_zooniverse_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
