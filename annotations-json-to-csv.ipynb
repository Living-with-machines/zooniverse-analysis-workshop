{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zooniverse JSON results to CSV\n",
    "\n",
    "This Notebook is designed to: #TODO description.\n",
    "\n",
    "## Assumptions about running this Notebook\n",
    "\n",
    "We created this Notebook on Apple Macs with Python installed as xxx. If you tweak the Notebook for Python installed via Anaconda or on a different operating system, please feel free to fork and share your changes.\n",
    "\n",
    "## Making a local copy and installing prerequisites\n",
    "\n",
    "TODO: How to make a local copy of the notebook, in order to run it on your local computer.\n",
    "\n",
    "This notebook uses the package `pandas`, which is not built-in to Python. If you don't already have it installed, you will need to run the following command in your command line:\n",
    "\n",
    "```sh\n",
    "$ pip install pandas\n",
    "```\n",
    "\n",
    "## Downloading Zooniverse data\n",
    "\n",
    "### 1. Log in\n",
    "\n",
    "First, [log in to Zooniverse](https://www.zooniverse.org/accounts/sign-in) as you normally would.\n",
    "\n",
    "### 2. Navigate to lab page\n",
    "\n",
    "Next, [navigate to the Zooniverse lab page](https://www.zooniverse.org/lab). On the page, you will see the projects where you are a collaborator:\n",
    "\n",
    "<img src=\"images/annotations-json-to-csv/1-zooniverse-lab.png?raw=true\" alt=\"A screenshot of the Zooniverse lab page\" style=\"max-width:75%;\" title=\"Screenshot of Zooniverse lab page\" />\n",
    "\n",
    "### 3. Navigate to \"Data Exports\"\n",
    "\n",
    "Next, click \"Data Exports\" in the right-hand menu bar:\n",
    "\n",
    "<img src=\"images/annotations-json-to-csv/2-data-exports.png?raw=true\" alt=\"A screenshot that shows where in the right-hand menu bar you will find the Data Exports option\" style=\"max-width:75%;\" title=\"Screenshot of Zooniverse lab splash page\" />\n",
    "\n",
    "### 4. Request relevant exports\n",
    "\n",
    "Next, you will want to press the two buttons for \"Request new classification export\" and \"Request new subject export\":\n",
    "\n",
    "<img src=\"images/annotations-json-to-csv/3-data-exports-requests.png?raw=true\" alt=\"A screenshot that shows the relevant buttons to press on the Zooniverse Data Exports page\" style=\"max-width:75%;\" title=\"Screenshot of Zooniverse Data Exports page\" />\n",
    "\n",
    "### 5. Await the completed download\n",
    "\n",
    "Your export might take a little while, but you will receive an email once your request is completed:\n",
    "\n",
    "<img src=\"images/annotations-json-to-csv/4-confirmation-email.png?raw=true\" alt=\"A screenshot that shows the confirmation email received when data export request has been completed\" style=\"max-width:75%;\" title=\"Screenshot of the confirmation email\" />\n",
    "\n",
    "### 6. Download the relevant files\n",
    "\n",
    "Note that clicking the link \"Download from your lab data exports page\" in the confirmation email will only take you back to the Data Exports page.\n",
    "\n",
    "Once you are back to the page, you need to use the correct links on the page to download the relevant CSV files:\n",
    "\n",
    "<img src=\"images/annotations-json-to-csv/5-request-completed.png?raw=true\" alt=\"A screenshot that shows the data export request completed on the Data Exports page\" style=\"max-width:75%;\" title=\"Screenshot of the Data Exports page with the relevant links\" />\n",
    "\n",
    "For each of those two files, right-click and choose \"Save Link As...\"\n",
    "\n",
    "<img src=\"images/annotations-json-to-csv/6-save-link-as.png?raw=true\" alt=\"A screenshot that shows the popup menu to save CSV file from your browser in the correct directory\" style=\"max-width:30%;\" title=\"Screenshot of the menu that shows the option Save Link As...\" />\n",
    "\n",
    "Finally, you can save the file wherever you want on your computer, but remember the correct path to the files as you will need to fill them out in the next cell! _We suggest that you save the files with the names `classifications.csv` and `subjects.csv` in your `Downloads` folder._\n",
    "\n",
    "<img src=\"images/annotations-json-to-csv/7-rename.png?raw=true\" alt=\"A screenshot that shows the renaming process of the CSV file\" style=\"max-width:30%;\" title=\"Screenshot of the download file dialog\" />\n",
    "\n",
    "Now, in the following cell, which is a Notebook cell registering your variable, we need you to fill in the path to the classifications file (as the `classifications_file` variable) and the subject file (as the `subjects_file` variable).\n",
    "\n",
    "As an example, if you saved the `classifications.csv` and `subjects.csv` files in the `Downloads` folder above (like in the screenshot), you might want to change the lines below to:\n",
    "\n",
    "```py\n",
    "classifications_file = \"/Users/<your-username>/Downloads/classifications.csv\"\n",
    "subjects_file = \"/Users/<your-username>/Downloads/subjects.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_file = \"test-files/test_classifications.csv\"\n",
    "subjects_file = \"test-files/test_subjects.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our definitions\n",
    "\n",
    "Now, we are ready to get started working with our data.\n",
    "\n",
    "First, we need to import the packages that we are going to use in the script below. Most of them are built-in to Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap, OrderedDict\n",
    "import hashlib  \n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our main data\n",
    "\n",
    "Next, it's time to read in the main CSV data as a pandas DataFrame, using the handy method `.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(classifications_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a handy method to look at the data that we have just imported, called `.head(num)`, where `num` should be replaced with the amount of rows in the frame that you want to see.\n",
    "\n",
    "We run it below to see what our imported data looks like unprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in our preview, the `classification_id` is a column, which is unique for each classification. It can thus be used as an \"index\" for the DataFrame, a nice way of querying the frame by individual IDs.\n",
    "\n",
    "Here's how we can set the index on our `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"classification_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Python and Pandas are both \"succeeding silently\", it can be good to sometimes have what we call a \"sanity check\" to ensure that we have the result that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure correct encoding\n",
    "\n",
    "Next, we will want to have a look at the encoding of each column.\n",
    "\n",
    "In the \"sanity check\" above, you can see that the columns `subject_data`, `annotations`, and `metadata` are structured as JSON (JavaScript Object Notation), which the DataFrame parser cannot interpret on its own, which is why we have to help it using the following parsing of those specific columns.\n",
    "\n",
    "If you use Pandas' `.apply()` method (which can be applied on a column or on the entire DataFrame), you can help the software interpret the data in the columns. Here, we pass a function (`json.loads`, i.e. the `loads` function from Python's built-in `json` package) to each of the columns containing JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"metadata\"] = df[\"metadata\"].apply(json.loads)\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we might want to do a \"sanity check\". To look at a specific part of a dataframe we can \"slice it\" by passing it a list (note the double `[[` and `]]` on each side of the selector) of the columns we want to look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"annotations\", \"metadata\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!\n",
    "\n",
    "In the following two sections, we will process the `metadata`, and `annotations` column respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract classification metadata (`metadata`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to do something similar to what we did with the subjects above to process the metadata for each classification.\n",
    "\n",
    "Because the data in the `metadata` column has nested JSON data (that is, objects and lists that are wrapped inside each other), we want to use the particular `json_normalize` method.\n",
    "\n",
    "Here's an example of what one `metadata` row looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.metadata[df.metadata.head(1).index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a separate DataFrame from the normalised JSON data in the columns using the json_normalize method and use `.head()` to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.json_normalize(df[\"metadata\"])\n",
    "\n",
    "df_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the shape of the classification metadata DataFrame (`df_metadata`) and the main classification DataFrame (`df`) are the same, we can apply the `set_index` method to the metadata to get the `classification_id` as index on the `df_metadata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.set_index(df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check time again — good practice, keep track of what you're doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract annotations (`annotations`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final information we want to extract and process from the main DataFrame are the annotations for each classification.\n",
    "\n",
    "In order to do so, we must first preprocess the JSON data that we have available, extracting a list of only the information we want (the \"annotation values\"). We set up a function that processes each individual row, and then apply it to each row, using the `.apply` method, which we have used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation_values(annotation_row):\n",
    "    \"\"\"\n",
    "    Takes an annotation row, which contains a list of tasks with values in dictionary {task, task_label, value}\n",
    "    and extracts the `value` for each `task`, disregarding the `task_label` and returns them as a dictionary,\n",
    "    for easy insertion into a DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_dictionaries = [{task_data.get('task'): task_data.get('value')} for task_data in annotation_row]\n",
    "    return dict(ChainMap(*extracted_dictionaries))\n",
    "\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(extract_annotation_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similarly to section 4 above, we loop through each row (of JSON data - contained in `json_data`) and extract the classification_id and annotations for each of them, which we then add on our new DataFrame `df_annotations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This cell generates deprecation warning (append is deprecated, use pd.concat instead)\n",
    "\n",
    "df_annotations = pd.DataFrame()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    s = pd.Series(row.annotations, name=index)\n",
    "    df_annotations = df_annotations.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the `.head()` method again to check our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Zooniverse subjects (`subject_data`)\n",
    "\n",
    "Next, we want to read in the `subjects.csv` file above, so we know what files each of the classifications were done on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects = pd.read_csv(subjects_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at our data using the `.head()` method that we learned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the classifications, we can see from this preview that the `subject_id` is a column, which is unique for each subject. Similarly to what we did with the classifications above, we can turn it into an \"index\" for the DataFrame, a nice way of querying the frame by individual IDs. Again, here's how we'd do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.set_index(\"subject_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the preview, there are some quirky aspects of the `subjects.csv` file from Zooniverse. Two things we might want to address here is that the `metadata` column is formatted as embedded JSON data, and so is the `locations` column.\n",
    "\n",
    "The `metadata` column corresponds to the data from any manifest file that you uploaded with your subject sets, so it often contains valuable information for us to have with our classifications.\n",
    "\n",
    "The `locations` column contains the URL to all of the images that each subject links to. For our projects, we might just have one location, but not necessarily: subjects can contain multiple images!\n",
    "\n",
    "For our purposes, we want to create a more readable list of locations and extract the metadata into its own columns. Let's go ahead and do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by making sure that both columns are formatted correctly, as we did with the classifications' metadata and annotations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects[\"metadata\"] = df_subjects[\"metadata\"].apply(json.loads)\n",
    "df_subjects[\"locations\"] = df_subjects[\"locations\"].apply(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the `locations` column. Since the data is structured as a Python dictionary `{id: \"URL\"}`, we can get a list of all the URLs by using Python's `dict` type's built-in `.values()` method. Here, we put that method into a custom function, which takes any row from the `subjects` DataFrame, makes a list from its values, and then joins them together with a comma separation (see the `\", \".join()` syntax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_as_list(row):\n",
    "    return \", \".join(list(row.values()))\n",
    "\n",
    "df_subjects[\"locations_list\"] = df_subjects[\"locations\"].apply(locations_as_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to save some space, we can remove the old `locations` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects = df_subjects.drop([\"locations\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will turn to the subjects' `metadata` column.\n",
    "\n",
    "Similarly to the extraction of classifications metadata above, we will create a separate DataFrame from the normalised JSON data from the subjects column using the `json_normalize` method and then use `.head()` to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects_metadata = pd.json_normalize(df_subjects[\"metadata\"])\n",
    "\n",
    "df_subjects_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since we know that the shape of the subjects' metadata DataFrame (`df_subjects_metadata`) and the main subject DataFrame (`df_subject`) are the same, we can apply the `set_index` method to the metadata to get the `subject_id` as index on the `df_subjects_metadata`.\n",
    "\n",
    "After, we'll run the `.head()` sanity check, to make sure all is well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects_metadata.set_index(df_subjects.index, inplace=True)\n",
    "\n",
    "df_subjects_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've now extracted the `metadata` column into a separate DataFrame, let's go ahead and remove the column from the original or \"main\" `df_subjects` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects = df_subjects.drop(\"metadata\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really great feature of DataFrames is that you can join them back together, as long as the `pandas` library has an easy way of matching the two DataFrames (or more!) together. A joined index, such as we have for `df_subjects` and `df_subjects_metadata` is a great way to do so.\n",
    "\n",
    "In order to get one large `df_subjects` DataFrame that contains all the data for each subject, we can thus run the `.join` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects = df_subjects.join(df_subjects_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see that `pandas` succeeds silently, so let's do a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is one large (but handy) DataFrame!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining all the data back together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final step, we want to join the data back together into a new main DataFrame, `df_final`, from which we will remove all the personal data so the dataset can be shared publicly, if we want.\n",
    "\n",
    "To recap, here are the four DataFrames that we have created thus far:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main DataFrame (`df`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subjects DataFrame (`df_subjects`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata DataFrame (`df_metadata`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations DataFrame (`df_annotations`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop existing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the summary above, of the main DataFrame (`df`), it now doubles the information in `df_subjects`, `df_metadata`, and `df_annotations`. So before we do anything else, we will drop the columns that we have now processed from `df` to reduce the size of `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"subject_data\", \"metadata\", \"annotations\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redact personal information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to ensure that we have anonymised the user data.\n",
    "\n",
    "Here, we have created our own function, `redact_username`, which uses a cryptographic hash method from Python's built-in `hashlib` library called `sha256`. You can read more about [the algorithm's history on Wikipedia](https://en.wikipedia.org/wiki/SHA-2) if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_username(row):\n",
    "    return hashlib.sha256(str(row).encode()).hexdigest() if not pd.isna(row) else None\n",
    "    \n",
    "df[\"user_name_redacted\"] = df[\"user_name\"].apply(redact_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also be interested in knowing whether a user was logged in or not when they annotated the subject. In order to preserve that information, we create another custom function, `user_was_logged_in`, which uses the information from the `user_name` column (which has a value like `not-logged-in-98ff168ef257e2fd9d4a` if the user was not logged in) to extract a `True` or `False` value (also called a \"boolean\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_was_logged_in(row):\n",
    "    return \"not-logged-in\" not in row if not pd.isna(row) else False\n",
    "\n",
    "df[\"user_logged_in\"] = df[\"user_name\"].apply(user_was_logged_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to drop columns that contain personal identifying information - usernames, IDs and IP addresses - from the main DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"user_id\", \"user_name\", \"user_ip\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all the data\n",
    "\n",
    "Now, it's time to join all the DataFrames into one new `df_final` frame.\n",
    "\n",
    "Adding the metadata and the annotations on the main `df` is an easy thing, since all of them share index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.join(df_metadata).join(df_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure that all the data is there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. But now we want to join our subjects on the `df_final` table as well. It's not entirely as easy as the previous multiple join task that we just did here.\n",
    "\n",
    "This time, because we're not using data points that are uniquely connected, one-to-one (one subject, in fact, will have many classifications), we will end up with duplicated information in the table. In order to join the main DataFrame with the subject DataFrame `df_subjects`, we need to make one intermediary step: We need to make both index columns available in each of the DataFrames.\n",
    "\n",
    "_Why do we need to make `df_final`'s index a column?_ We will _merge_ two DataFrames, which is a function that will discard both indices of the merging DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects[\"subject_ids\"] = df_subjects.index\n",
    "df_final[\"classification_id\"] = df_final.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the merge, we also want to make sure that the two matching columns on both DataFrames are of the same (correct) type—otherwise Pandas will have trouble finding the matching subject row for each classification.\n",
    "\n",
    "For that, we use the built-in method on each DataFrame, `.astype()` which \"casts\" a column as a type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subjects[\"subject_ids\"] = df_subjects[\"subject_ids\"].astype(int)\n",
    "df_final[\"subject_ids\"] = df_final[\"subject_ids\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be able to use the `merge` function from Pandas central library to join the two on the column that is shared between the two DataFrames (in our case `subject_ids`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_final, df_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can make the `classification_id` column an index again, on our final DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.set_index(\"classification_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last sanity check, before we're ready to export!\n",
    "\n",
    "This time, however, we set the max column to be displayed to `None` because we're interested in seeing all the columns. Then we use our now familiar `.head()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export!\n",
    "\n",
    "In this final step, we can export the file to whatever format that we'd like to use.\n",
    "\n",
    "In the case below, we opt for `.csv`, which is a common, open file format that can be easily opened in Microsoft Excel for further processing. It can also be imported into visualisation software, such as Power BI, or websites like ObservableHQ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"annotations_unpacked_no_snippets.csv\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
